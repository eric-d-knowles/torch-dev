#!/usr/bin/env bash
set -euo pipefail

clear

# --- Config ---
CONFIG_DIR="$HOME/.config/torch"
PREFS_FILE="$CONFIG_DIR/last_job_prefs"
SSH_CONFIG="$HOME/.ssh/config"
mkdir -p "$CONFIG_DIR"

# --- Detect local and cluster usernames (avoid hardcoding) ---
LOCAL_USER="${USER:-$(whoami)}"
CLUSTER_USER=""

# Try to read cluster username from SSH config for host 'torch'
if [[ -f "$SSH_CONFIG" ]] && grep -qE '^Host[[:space:]]+torch$' "$SSH_CONFIG" 2>/dev/null; then
    CLUSTER_USER=$(awk '/^Host[[:space:]]+torch$/,/^Host /{ if ($1=="User") print $2 }' "$SSH_CONFIG" | head -n1 | tr -d '\r' || true)
fi

# If not found, try asking the remote for its effective user
if [[ -z "$CLUSTER_USER" ]]; then
    CLUSTER_USER=$(ssh torch 'printf "%s" "$USER"' 2>/dev/null || true)
fi

# If still empty, prompt the local user
if [[ -z "$CLUSTER_USER" ]]; then
    read -p "Cluster username for SSH host 'torch' (default: $LOCAL_USER): " input_cluster_user
    CLUSTER_USER="${input_cluster_user:-$LOCAL_USER}"
fi

# --- Cleanup on abort ---
cleanup() {
    printf '\033[1;31mAborted. Cleaning up...\033[0m\n'
    ssh torch "scancel -u $CLUSTER_USER 2>/dev/null || true"
    exit 1
}
trap cleanup INT TERM

# --- Ensure SSH control socket is active ---
echo -e "\033[1;34mChecking SSH connection to Torch...\033[0m"
if ! ssh -O check torch 2>/dev/null; then
    echo "Control socket not active. Please authenticate:"
    ssh -fNM torch
    echo "Authenticated."
fi
echo

# --- Defaults ---
DEFAULT_TIME_HOURS=2
DEFAULT_PARTITION=""
DEFAULT_CPUS=4
DEFAULT_RAM=32G
DEFAULT_GPU=no
DEFAULT_PROJECT=""
DEFAULT_ACCOUNT="torch_pr_217_general"

# Load previous values
[[ -f "$PREFS_FILE" ]] && source "$PREFS_FILE"

TIME_HOURS="${TIME_HOURS:-$DEFAULT_TIME_HOURS}"
PARTITION="${PARTITION:-$DEFAULT_PARTITION}"
CPUS="${CPUS:-$DEFAULT_CPUS}"
RAM="${RAM:-$DEFAULT_RAM}"
GPU="${GPU:-$DEFAULT_GPU}"
PROJECT="${PROJECT:-$DEFAULT_PROJECT}"
ACCOUNT="${ACCOUNT:-$DEFAULT_ACCOUNT}"

# --- Prompt for preferences ---
printf '\033[1;34mResource request:\033[0m\n'
read -p "  Account (default: $ACCOUNT): " input_account
read -p "  Hours (default: $TIME_HOURS): " input_time
read -p "  Partition [leave blank for default]: " input_partition
read -p "  CPUs (default: $CPUS): " input_cpus
read -p "  RAM (default: $RAM): " input_ram
read -p "  GPU? [yes/no] (default: $GPU): " input_gpu
read -p "  Project path under /scratch/$CLUSTER_USER/ (default: $PROJECT): " input_project
echo

[[ -n "$input_account" ]] && ACCOUNT="$input_account"
[[ -n "$input_time" ]] && TIME_HOURS="$input_time"
[[ -n "$input_partition" ]] && PARTITION="$input_partition"
[[ -n "$input_cpus" ]] && CPUS="$input_cpus"
[[ -n "$input_ram" ]] && RAM="$input_ram"
[[ -n "$input_gpu" ]] && GPU="$input_gpu"
[[ -n "$input_project" ]] && PROJECT="$input_project"

# Save prefs
cat > "$PREFS_FILE" <<EOF
TIME_HOURS=$TIME_HOURS
PARTITION=$PARTITION
CPUS=$CPUS
RAM=$RAM
GPU=$GPU
PROJECT="$PROJECT"
ACCOUNT="$ACCOUNT"
EOF

# Build project path
if [[ -n "$PROJECT" ]]; then
    WORK_DIR="/scratch/$CLUSTER_USER/$PROJECT"
else
    WORK_DIR="/scratch/$CLUSTER_USER"
fi

# --- Clean up old jobs ---
echo -e "\033[1;34mCleaning up old jobs...\033[0m"
ssh torch "scancel -u $CLUSTER_USER 2>/dev/null || true"
ssh torch "mkdir -p ~/.config/torch"

# --- Submit job ---
echo -e "\033[1;34mSubmitting job...\033[0m"

GPU_FLAG=""
[[ "$GPU" == "yes" ]] && GPU_FLAG="--gres=gpu:1"

PARTITION_FLAG=""
[[ -n "$PARTITION" ]] && PARTITION_FLAG="--partition=$PARTITION"

JOB_ID=$(ssh torch "sbatch --parsable \\
    --time=${TIME_HOURS}:00:00 \\
    --account=$ACCOUNT \\
    $PARTITION_FLAG \\
    --cpus-per-task=$CPUS \\
    --mem=$RAM \\
    $GPU_FLAG \\
    --wrap='sleep infinity'")

echo "Submitted job $JOB_ID"

# --- Wait for allocation ---
echo -e "\033[1;34mWaiting for compute node...\033[0m"
for i in {1..300}; do
    COMPUTE_NODE=$(ssh torch "squeue -j $JOB_ID -h -o '%N' 2>/dev/null | grep -v '^$'" || true)
    if [[ -n "$COMPUTE_NODE" ]]; then
        break
    fi
    sleep 1
    printf "."
done
echo

if [[ -z "$COMPUTE_NODE" ]]; then
    echo -e "\033[1;31mTimed out waiting for allocation.\033[0m"
    echo "Check job status: ssh torch 'squeue -u $CLUSTER_USER'"
    exit 1
fi

echo -e "\033[1;32mAllocated: $COMPUTE_NODE\033[0m"

# --- Update SSH config ---
echo -e "\033[1;34mUpdating SSH config...\033[0m"

# Remove old torch-compute entry
sed -i.bak '/^Host torch-compute$/,/^Host /{ /^Host torch-compute$/d; /^Host /!d; }' "$SSH_CONFIG" 2>/dev/null || true

# Append new entry
cat >> "$SSH_CONFIG" <<EOF

Host torch-compute
    HostName $COMPUTE_NODE
    User $CLUSTER_USER
    ProxyJump torch
    ForwardAgent yes
    StrictHostKeyChecking no
    UserKnownHostsFile /dev/null
EOF

# --- Wait for compute node SSH ---
echo -e "\033[1;34mWaiting for SSH on compute node...\033[0m"
for i in {1..30}; do
    if ssh -o ConnectTimeout=2 torch-compute 'true' 2>/dev/null; then
        break
    fi
    sleep 2
    printf "."
done
echo

# --- Launch VS Code ---
echo -e "\033[1;32mLaunching VS Code...\033[0m"
if command -v code >/dev/null 2>&1; then
    code --folder-uri "vscode-remote://ssh-remote+torch-compute${WORK_DIR}"
else
    echo -e "\033[1;33m'code' CLI not found; please install it to open VS Code from the terminal.\033[0m"
    echo "From Visual Studio Code: open the Command Palette and run:" 
    echo "  Shell Command: Install 'code' command in PATH"
    echo
    echo "Or install VS Code via Homebrew (macOS):"
    echo "  brew install --cask visual-studio-code"
    echo
    echo "After installing, run:" 
    echo "  code --folder-uri \"vscode-remote://ssh-remote+torch-compute${WORK_DIR}\""
    echo
    echo "You can also open the following URI manually in VS Code:" 
    echo "  vscode-remote://ssh-remote+torch-compute${WORK_DIR}"
fi

echo
echo -e "\033[1;34mSession info:\033[0m"
echo -e "  Job ID: \033[1;33m$JOB_ID\033[0m"
echo -e "  Compute node: \033[1;33m$COMPUTE_NODE\033[0m"
echo -e "  Work dir: \033[1;33m$WORK_DIR\033[0m"
echo -e "  Time limit: \033[1;33m${TIME_HOURS}h\033[0m"
echo
echo -e "To reconnect: \033[1;33mcode --folder-uri \"vscode-remote://ssh-remote+torch-compute${WORK_DIR}\"\033[0m"
echo -e "To cancel: \033[1;33mssh torch 'scancel $JOB_ID'\033[0m"
